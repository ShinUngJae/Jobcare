{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K7vrXQH9bqS"
      },
      "source": [
        "## 목차\n",
        "#### 1. 라이브러리 설치 및 불러오기\n",
        "#### 2. 데이터 전처리\n",
        "#### 3. 모델링\n",
        "#### 4. 스태킹\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am-OWOdxHMtC"
      },
      "source": [
        "## 1. 라이브러리 설치 및 불러오기 (Install and Load the Libraries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6cus4a3HErO",
        "outputId": "b4dd117f-dac9-48e6-d774-f5f6ff03a561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 시스템 \n",
        "import os, sys\n",
        "import timeit\n",
        "\n",
        "# 기본 라이브러리\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import math \n",
        "\n",
        "# 문자열 처리\n",
        "import string\n",
        "\n",
        "# 시각화 라이브러리\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# 구글 드라이브 \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 모델링 라이브러리\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold , KFold\n",
        "from sklearn.metrics import *\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "!pip install -q catboost \n",
        "from catboost import CatBoostClassifier \n",
        "\n",
        "import pickle, joblib\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBa4SIOTHXBY"
      },
      "source": [
        "## 2. 데이터 전처리 (Data Preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNmhGRRigqaw"
      },
      "source": [
        "### 3종류의 데이터프레임을 만들기\n",
        "- **[1타입]**:  train1  \n",
        "- **[2타입]**: train2  \n",
        "- **[3타입]**: train3  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38j7BY_tgQC2"
      },
      "source": [
        "### **2-1. 1타입 데이터 만들기**\n",
        "#### 2-1-1. 파생변수 생성\n",
        "1. D / H / L 코드의 매칭 여부\n",
        "  - D & L은 세분류, 대분류 기준 / H는 중분류, 대분류 기준 매칭시키기\n",
        "  - d_l_match_123, d_m_match_123, d_s_match_123, h_l_match_123, h_m_match_123, h_s_match_123\n",
        "    - ex) d_l_match_123: 1,2,3 중 하나라도 대분류가 match 된다면 True 아니라면 False\n",
        "\n",
        "#### 2-1-2. 변수형 변환\n",
        "\n",
        "#### 2-1-3.  id 리스트 저장\n",
        "\n",
        "#### 2-1-4. 불필요한 컬럼 제거\n",
        "- EDA 결과 `person_prefer_f` 열과 `person_prefer_g` 열은 모든 샘플에 대해 동일한 값을 가진 상수라는 것을 파악할 수 있었다. 따라서 해당 열은 제거하고 분석을 진행하기로 하였다.\n",
        "- `contents_open_dt`를 제외할 때 모델의 성능이 더 좋았기 때문에 해당 열을 제외했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcom-QGjiypw"
      },
      "source": [
        "#### `preprocessing_1` 전처리 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M9PczsHwBruO"
      },
      "outputs": [],
      "source": [
        "def preprocessing_1(df):\n",
        "\n",
        "  # 1) D / H / L 코드 파생변수 생성  \n",
        "  D_code_1=D_code.rename(columns={'속성 D 코드':'person_prefer_d_1','속성 D 세분류코드':'person_prefer_d_1_m','속성 D 대분류코드':'person_prefer_d_1_l'}, inplace = False).drop(['속성 D 소분류코드','속성 D 중분류코드'],axis=\"columns\")\n",
        "  D_code_2=D_code.rename(columns={'속성 D 코드':'person_prefer_d_2','속성 D 세분류코드':'person_prefer_d_2_m','속성 D 대분류코드':'person_prefer_d_2_l'}, inplace = False).drop(['속성 D 소분류코드','속성 D 중분류코드'],axis=\"columns\")\n",
        "  D_code_3=D_code.rename(columns={'속성 D 코드':'person_prefer_d_3','속성 D 세분류코드':'person_prefer_d_3_m','속성 D 대분류코드':'person_prefer_d_3_l'}, inplace = False).drop(['속성 D 소분류코드','속성 D 중분류코드'],axis=\"columns\")\n",
        "  D_code_contents=D_code.rename(columns = {'속성 D 코드' : 'contents_attribute_d','속성 D 세분류코드':'contents_attribute_d_m','속성 D 대분류코드':'contents_attribute_d_l'}, inplace = False).drop(['속성 D 소분류코드','속성 D 중분류코드'],axis=\"columns\")\n",
        "  H_code_1=H_code.rename(columns={'속성 H 코드':'person_prefer_h_1','속성 H 중분류코드':'person_prefer_h_1_m','속성 H 대분류코드':'person_prefer_h_1_l'}, inplace = False)\n",
        "  H_code_2=H_code.rename(columns={'속성 H 코드':'person_prefer_h_2','속성 H 중분류코드':'person_prefer_h_2_m','속성 H 대분류코드':'person_prefer_h_2_l'}, inplace = False)\n",
        "  H_code_3=H_code.rename(columns={'속성 H 코드':'person_prefer_h_3','속성 H 중분류코드':'person_prefer_h_3_m','속성 H 대분류코드':'person_prefer_h_3_l'}, inplace = False)\n",
        "  H_code_contents=H_code.rename(columns={'속성 H 코드' : 'contents_attribute_h','속성 H 중분류코드':'contents_attribute_h_m','속성 H 대분류코드':'contents_attribute_h_l'}, inplace = False)\n",
        "  L_code_contents=L_code.rename(columns={'속성 L 코드' : 'contents_attribute_l','속성 L 세분류코드':'contents_attribute_l_m','속성 L 대분류코드':'contents_attribute_l_l'}, inplace = False).drop(['속성 L 소분류코드','속성 L 중분류코드'],axis=\"columns\")\n",
        "\n",
        "  df_list=[D_code_1,D_code_2,D_code_3,D_code_contents,\n",
        "          H_code_1,H_code_2,H_code_3,H_code_contents, L_code_contents]\n",
        "  df_column=[\"person_prefer_d_1\",\"person_prefer_d_2\",\"person_prefer_d_3\",\"contents_attribute_d\",\n",
        "            \"person_prefer_h_1\",\"person_prefer_h_2\",\"person_prefer_h_3\",\"contents_attribute_h\",\n",
        "            \"contents_attribute_l\"]\n",
        "\n",
        "  for i in range(0,len(df_column)):\n",
        "    df = pd.merge(df, df_list[i],on=df_column[i]) \n",
        "\n",
        "  df.loc[(df['person_prefer_d_1_m']==df['contents_attribute_d_m'])|(df['person_prefer_d_2_m']==df['contents_attribute_d_m'])| (df['person_prefer_d_3_m']==df['contents_attribute_d_m']),'d_m_match_123']=True\n",
        "  df.loc[(df['person_prefer_d_1_l']==df['contents_attribute_d_l'])|(df['person_prefer_d_2_l']==df['contents_attribute_d_l'])| (df['person_prefer_d_3_l']==df['contents_attribute_d_l']),'d_l_match_123']=True\n",
        "  df.loc[(df['person_prefer_d_1']==df['contents_attribute_d'])|(df['person_prefer_d_2']==df['contents_attribute_d'])| (df['person_prefer_d_3']==df['contents_attribute_d']),'d_s_match_123']=True\n",
        "  df.loc[(df['person_prefer_h_1_l']==df['contents_attribute_h_l'])|(df['person_prefer_h_2_l']==df['contents_attribute_h_l'])| (df['person_prefer_h_3_l']==df['contents_attribute_h_l']),'h_l_match_123']=True\n",
        "  df.loc[(df['person_prefer_h_1_m']==df['contents_attribute_h_m'])|(df['person_prefer_h_2_m']==df['contents_attribute_h_m'])| (df['person_prefer_h_3_m']==df['contents_attribute_h_m']),'h_m_match_123']=True\n",
        "  df.loc[(df['person_prefer_h_1']==df['contents_attribute_h'])|(df['person_prefer_h_2']==df['contents_attribute_h'])| (df['person_prefer_h_3']==df['contents_attribute_h']),'h_s_match_123']=True\n",
        "  \n",
        "  df = df.fillna(False).sort_values(by=['id'],axis=0) \n",
        "\n",
        "\n",
        "  # 2) 변수형 변환\n",
        "  df.loc[:, df.columns != 'contents_open_dt'] = \\\n",
        "      df.loc[:, df.columns != 'contents_open_dt'].astype('int') # boolean -> int\n",
        "  numeric_columns = ['contents_rn','person_rn','contents_attribute_j'\n",
        "                     ]\n",
        "  categorical_columns = list(df.columns.drop(numeric_columns))\n",
        "  df=df[categorical_columns].astype('category')\n",
        "\n",
        "  # 3) id list 저장 \n",
        "  id_list = list(df['id'])\n",
        "  df = df.sort_values(by=['id'],axis=0).set_index('id')\n",
        "\n",
        "\n",
        "  # 4) 불필요한 컬럼 삭제 \n",
        "  df.drop(['person_prefer_f','person_prefer_g','contents_open_dt'],\n",
        "          axis=\"columns\",inplace=True)\n",
        "  \n",
        "  return id_list, df  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCBSAs-5kQEg"
      },
      "source": [
        "#### `preprocessing_1` 전처리 함수 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uUjaymAc5o-y"
      },
      "outputs": [],
      "source": [
        "# 디렉토리 설정: \"[DACON] 잡케어 추천 알고리즘 경진대회\" 폴더로 이동\n",
        "path = \"/content/drive/MyDrive/[DACON] 잡케어 추천 알고리즘 경진대회/data/\"\n",
        "os.chdir(path)\n",
        "\n",
        "# 파일 불러오기 \n",
        "train = pd.read_csv('Jobcare_data/train.csv')\n",
        "test = pd.read_csv('Jobcare_data/test.csv')\n",
        "D_code=pd.read_csv('Jobcare_data/속성_D_코드.csv')\n",
        "L_code=pd.read_csv('Jobcare_data/속성_L_코드.csv')\n",
        "H_code=pd.read_csv('Jobcare_data/속성_H_코드.csv')\n",
        "\n",
        "# 1타입 전처리 함수 적용하기 \n",
        "train1_idx, train1 = preprocessing_1(train)\n",
        "#train1.to_csv('train1.csv', header=True,index=False)\n",
        "\n",
        "test1_idx, test1 = preprocessing_1(test)  \n",
        "#test1.to_csv('test1.csv', header=True,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "U7RhzyGxbfan"
      },
      "outputs": [],
      "source": [
        "#train1 = pd.read_csv('train1.csv')  #train_new_week.csv\n",
        "numeric_columns1 = ['contents_rn','person_rn','contents_attribute_j']\n",
        "categorical_columns1 = list(train1.columns.drop(['target']))\n",
        "\n",
        "feature_names1 = numeric_columns1 + categorical_columns1\n",
        "f_labels1 = numeric_columns1 + categorical_columns1\n",
        "confusion_lbs1 = ['Not Used', 'Used']\n",
        "\n",
        "f1_scores1=[]\n",
        "models1=[]\n",
        "\n",
        "# X, y 나누기 (train에만 적용)\n",
        "y_train1, X_train1 = train1['target'], train1.drop(['target'],axis=1)\n",
        "X_train1 = X_train1[categorical_columns1].astype('category')\n",
        "primary_eval_metric1 = f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR-pnzvZkSBz"
      },
      "source": [
        "### **2-2. 2타입 데이터 만들기**\n",
        "\n",
        "#### 2-2-1. 파생변수 만들기\n",
        "1. 코드 매칭 관련\n",
        "2. person-contents 일치 여부 관련 \n",
        "\n",
        "#### 2-2-2. id 리스트 저장\n",
        "\n",
        "#### 2-2-3. 불필요한 컬럼 제거\n",
        "- 'id','contents_rn','person_rn','contents_open_dt', 'person_prefer_f','person_prefer_g', 'd_l_match_yn','d_m_match_yn','d_s_match_yn','h_l_match_yn','h_m_match_yn','h_s_match_yn'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXsmqWYspkQm"
      },
      "source": [
        "#### `preprocessing_2` 전처리 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gYQ-UVsBlkP9"
      },
      "outputs": [],
      "source": [
        "def preprocessing_2(df):\n",
        "\n",
        "  # 1-1) 코드 매칭 관련 파생변수 생성 (add_code)\n",
        "  D_attr_types = ['세', '소', '중', '대']\n",
        "  H_attr_types = ['중', '대']\n",
        "  L_attr_types = ['세', '소', '중', '대']\n",
        "  prefer_rank = ['1', '2', '3'] \n",
        "\n",
        "  ## D 속성 코드 \n",
        "  for D_type in D_attr_types:\n",
        "    # 회원 속성 D\n",
        "    for rank in prefer_rank:\n",
        "      df['person_prefer_d_{}_{}'.format(rank, D_type)] = df['person_prefer_d_{}'.format(rank)].apply(lambda x:attr_D_code[x]['속성 D {}분류코드'.format(D_type)])\n",
        "    # 컨텐츠 속성 D \n",
        "    df['contents_attribute_d_{}'.format(D_type)] = df['contents_attribute_d'].apply(lambda x:attr_D_code[x]['속성 D {}분류코드'.format(D_type)])\n",
        "\n",
        "  ## H 속성 코드 \n",
        "  for H_type in H_attr_types:\n",
        "    # 회원 속성 H\n",
        "    for rank in prefer_rank:\n",
        "      df['person_prefer_h_{}_{}'.format(rank, H_type)] = df['person_prefer_h_{}'.format(rank)].apply(lambda x:attr_H_code[x]['속성 H {}분류코드'.format(H_type)])\n",
        "    # 컨텐츠 속성 H \n",
        "    df['contents_attribute_h_{}'.format(H_type)] = df['contents_attribute_h'].apply(lambda x:attr_H_code[x]['속성 H {}분류코드'.format(H_type)])\n",
        "\n",
        "  ## L 속성 코드 \n",
        "  for L_type in L_attr_types:\n",
        "    # 컨텐츠 속성 L \n",
        "    df['contents_attribute_l_{}'.format(L_type)] = df['contents_attribute_l'].apply(lambda x:attr_L_code[x]['속성 L {}분류코드'.format(L_type)])\n",
        "\n",
        "\n",
        "  # 1-2) person-contents 일치 여부 파생변수 생성 \n",
        "  cols_equal = [\n",
        "          ('person_prefer_c','contents_attribute_c'),\n",
        "\n",
        "          ('person_prefer_d_1_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_1_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_1_소','contents_attribute_d_소'),\n",
        "          ('person_prefer_d_1_세','contents_attribute_d_세'),\n",
        "          ('person_prefer_d_1','contents_attribute_d'),\n",
        "\n",
        "          ('person_prefer_d_2_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_2_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_2_소','contents_attribute_d_소'),\n",
        "\n",
        "          ('person_prefer_d_3_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_3_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_3_소','contents_attribute_d_소'),\n",
        "\n",
        "          ('person_prefer_h_1_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_1_중','contents_attribute_h_중'),\n",
        "          ('person_prefer_h_1','contents_attribute_h'),\n",
        "\n",
        "          ('person_prefer_h_2_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_2_중','contents_attribute_h_중'),\n",
        "\n",
        "          ('person_prefer_h_3_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_3_중','contents_attribute_h_중')\n",
        "        \n",
        "  ]\n",
        "\n",
        "  for col1, col2 in cols_equal:\n",
        "    df[f\"{col1}_{col2}\"]=(df[col1]==df[col2]).astype(int)\n",
        "\n",
        "  df['person_attribute_a_contents_attribute_a'] = np.where(((df['person_attribute_a'] == df['contents_attribute_a'])|(df['contents_attribute_a']==3)),1,0)  # EDA 결과 3인 경우는 '상관없음'으로 판단함\n",
        "  df['person_attribute_e_contents_attribute_e'] = np.where(((df['person_prefer_e'] == df['contents_attribute_e'])|(df['contents_attribute_e']==0)),1,0)  # EDA 결과 0인 경우는 '상관없음'으로 판단함 \n",
        "\n",
        "  # 2) id 저장 \n",
        "  id_list = list(df['id'])\n",
        "  df = df.set_index('id')\n",
        "\n",
        "  # 3) 불필요한 열 제거 \n",
        "  del_cols_list = [c for c in df.columns if c in \n",
        "                    ['id','contents_rn','person_rn','contents_open_dt',\n",
        "                     'person_prefer_f','person_prefer_g',\n",
        "                     'd_l_match_yn','d_m_match_yn','d_s_match_yn',\n",
        "                     'h_l_match_yn','h_m_match_yn','h_s_match_yn']\n",
        "  ]\n",
        "  df.drop(del_cols_list, axis=\"columns\",inplace=True) \n",
        "\n",
        "  return id_list, df \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jUfFLCLpm-0"
      },
      "source": [
        "#### `preprocessing_2` 전처리 함수 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UgkU3dLvwk_L"
      },
      "outputs": [],
      "source": [
        "# 파일 불러오기 \n",
        "train = pd.read_csv('Jobcare_data/train.csv')\n",
        "test = pd.read_csv('Jobcare_data/test.csv')\n",
        "attr_D_code = pd.read_csv(\"Jobcare_data/속성_D_코드.csv\", index_col=0).T.to_dict()\n",
        "attr_H_code = pd.read_csv(\"Jobcare_data/속성_H_코드.csv\", index_col=0).T.to_dict()\n",
        "attr_L_code = pd.read_csv(\"Jobcare_data/속성_L_코드.csv\", index_col=0).T.to_dict()\n",
        "\n",
        "# 2타입 전처리 함수 적용하기\n",
        "train2_idx, train2 = preprocessing_2(train) \n",
        "#train2.to_csv('train2.csv', header=True,index=False)\n",
        "\n",
        "test2_idx, test2 = preprocessing_2(test)\n",
        "#test2.to_csv('test2.csv', header=True,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iWZ2oFKd6EYU"
      },
      "outputs": [],
      "source": [
        "# 범주형 변수 지정 \n",
        "categorical_columns2 = test2.columns[test2.nunique() > 2].tolist()\n",
        "\n",
        "# X, y 나누기 (train만 적용)\n",
        "y_train2 = train2['target']\n",
        "X_train2 = train2.loc[:,train2.columns !='target'] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpWElqZ5rXW3"
      },
      "source": [
        "### 2-3. 3타입 데이터 만들기 \n",
        "\n",
        "#### 2-3-1. id 리스트 저장\n",
        "\n",
        "#### 2-3-2. 파생변수 만들기\n",
        "1. 코드 매칭 관련\n",
        "2. person-contents 일치 여부 관련 \n",
        "  - person_prefer과 contents_attribute이 일치하는지의 여부로 1/0 값을 부여했다.\n",
        "  - 회원이 선호하는 속성과 컨텐츠 속성, 이 두 가지를 비교하고자 하였는데, 주어진 데이터가 명목형 변수이므로 단순 사칙연산이 불가능할 뿐만 아니라 코드값이 나열된 형태를 고려했을 때 빼기 연산으로 두 속성 간의 차이가 반영되기 어렵다고 판단하여, 두 변수의 일치 여부만을 살펴보았다.\n",
        "  - 일치여부를 살펴본 쌍의 개수는 21개로, 21개의 새로운 변수를 생성했다.\n",
        "  - EDA 결과 `contents_attribute_a`가 3인 경우와 `contents_attribute_e`가 0인 경우는 `상관없음`으로 판단하여 전처리를 진행했다. \n",
        "3. cumct, diff 관련\n",
        "  - test 데이터에는 train 데이터에서 얻은 수치까지만을 활용하여 data leakage가 발생하지 않도록 하였다.\n",
        "    - ex) person_rn이 1인 샘플이 train 데이터에 4건 있었다면, test 데이터에서 person_rn이 1인 샘플에 모두 4라는 값이 들어가도록 한다.\n",
        "    - ex) person_rn이 train 데이터에는 없지만 test 데이터에는 있는 경우, 0으로 값을 채운다.\n",
        "  - 3-1) 누적도수 관련 열 생성: contents_open_D_cumct, contents_open_W_cumct, contents_open_M_cumct, person_open_D_cumct, person_open_W_cumct, person_open_M_cumct\n",
        "    - 각 콘텐츠가 일자별, 주차별, 월별로 얼마나 클릭되었는지(how many times the contents was opened)\n",
        "    - 각 이용자가 일자별, 주차별, 월별로 얼마나 클릭했는지(how many times the person opened the contents)\n",
        "  - 3-2) 시간차 관련 열 생성: contents_open_D_timediff, contents_open_W_timediff, contents_open_M_timediff, person_open_D_timediff, person_open_W_timediff, person_open_M_timediff\n",
        "    - 각 콘텐츠가 일자별, 주차별, 월별로 다음 열과의 차이(interval)\n",
        "    - 각 이용자가 일자별, 주차별, 월별로 열람한 시간차, 다음 열과의 차이(interval)\n",
        "\n",
        "#### 2-3-3. 불필요한 컬럼 제거\n",
        "- 'id','contents_rn','person_rn','contents_open_dt', 'person_prefer_f','person_prefer_g', 'd_l_match_yn','d_m_match_yn','d_s_match_yn','h_l_match_yn','h_m_match_yn','h_s_match_yn'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HDCro5zrXW3"
      },
      "source": [
        "#### `preprocessing_3` 전처리 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4sHwvtLVWR-_"
      },
      "outputs": [],
      "source": [
        "def preprocessing_3_train(df=train):\n",
        "\n",
        "  # 1) id 저장 \n",
        "  id_list = list(df['id'])\n",
        "\n",
        "  # 2-1) 코드 매칭 관련 파생변수 생성 (add_code)\n",
        "  D_attr_types = ['세', '소', '중', '대']\n",
        "  H_attr_types = ['중', '대']\n",
        "  L_attr_types = ['세', '소', '중', '대']\n",
        "  prefer_rank = ['1', '2', '3'] \n",
        "\n",
        "  ## D 속성 코드 \n",
        "  for D_type in D_attr_types:\n",
        "    # 회원 속성 D\n",
        "    for rank in prefer_rank:\n",
        "      df['person_prefer_d_{}_{}'.format(rank, D_type)] = df['person_prefer_d_{}'.format(rank)].apply(lambda x:attr_D_code[x]['속성 D {}분류코드'.format(D_type)])\n",
        "    # 컨텐츠 속성 D \n",
        "    df['contents_attribute_d_{}'.format(D_type)] = df['contents_attribute_d'].apply(lambda x:attr_D_code[x]['속성 D {}분류코드'.format(D_type)])\n",
        "\n",
        "  ## H 속성 코드 \n",
        "  for H_type in H_attr_types:\n",
        "    # 회원 속성 H\n",
        "    for rank in prefer_rank:\n",
        "      df['person_prefer_h_{}_{}'.format(rank, H_type)] = df['person_prefer_h_{}'.format(rank)].apply(lambda x:attr_H_code[x]['속성 H {}분류코드'.format(H_type)])\n",
        "    # 컨텐츠 속성 H \n",
        "    df['contents_attribute_h_{}'.format(H_type)] = df['contents_attribute_h'].apply(lambda x:attr_H_code[x]['속성 H {}분류코드'.format(H_type)])\n",
        "\n",
        "  ## L 속성 코드 \n",
        "  for L_type in L_attr_types:\n",
        "    # 컨텐츠 속성 L \n",
        "    df['contents_attribute_l_{}'.format(L_type)] = df['contents_attribute_l'].apply(lambda x:attr_L_code[x]['속성 L {}분류코드'.format(L_type)])\n",
        "\n",
        "\n",
        "  # 2-2) person-contents 일치 여부 파생변수 생성 \n",
        "  cols_equal = [\n",
        "          ('person_prefer_c','contents_attribute_c'),\n",
        "\n",
        "          ('person_prefer_d_1_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_1_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_1_소','contents_attribute_d_소'),\n",
        "          ('person_prefer_d_1_세','contents_attribute_d_세'),\n",
        "          ('person_prefer_d_1','contents_attribute_d'),\n",
        "\n",
        "          ('person_prefer_d_2_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_2_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_2_소','contents_attribute_d_소'),\n",
        "\n",
        "          ('person_prefer_d_3_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_3_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_3_소','contents_attribute_d_소'),\n",
        "\n",
        "          ('person_prefer_h_1_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_1_중','contents_attribute_h_중'),\n",
        "          ('person_prefer_h_1','contents_attribute_h'),\n",
        "\n",
        "          ('person_prefer_h_2_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_2_중','contents_attribute_h_중'),\n",
        "\n",
        "          ('person_prefer_h_3_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_3_중','contents_attribute_h_중')\n",
        "        \n",
        "  ]\n",
        "\n",
        "  for col1, col2 in cols_equal:\n",
        "    df[f\"{col1}_{col2}\"]=(df[col1]==df[col2]).astype(int)\n",
        "\n",
        "  df['person_attribute_a_contents_attribute_a'] = np.where(((df['person_attribute_a'] == df['contents_attribute_a'])|(df['contents_attribute_a']==3)),1,0)  # EDA 결과 3인 경우는 '상관없음'으로 판단함\n",
        "  df['person_attribute_e_contents_attribute_e'] = np.where(((df['person_prefer_e'] == df['contents_attribute_e'])|(df['contents_attribute_e']==0)),1,0)  # EDA 결과 0인 경우는 '상관없음'으로 판단함 \n",
        "\n",
        "  # 2-3) cumdiff 관련 파생변수 생성 \n",
        "\n",
        "  # 데이터형 변환: object -> datetime\n",
        "  df['contents_open_dt'] = pd.to_datetime(df['contents_open_dt'])\n",
        "  \n",
        "  # 2-3-1) cumct 관련 \n",
        "  intervals = ['D', 'W', 'M'] \n",
        "\n",
        "  # contents 기준\n",
        "  for i in intervals:\n",
        "    df['contents_open_{}_cumct'.format(i)] = df.sort_values('contents_open_dt'). \\\n",
        "                                  groupby([pd.Grouper('contents_rn'), pd.Grouper(freq=i, key='contents_open_dt')]). \\\n",
        "                                  cumcount()+1  \n",
        "  # person 기준\n",
        "  for i in intervals:\n",
        "    df['person_open_{}_cumct'.format(i)] = df.sort_values('contents_open_dt'). \\\n",
        "                                  groupby([pd.Grouper('person_rn'), pd.Grouper(freq=i, key='contents_open_dt')]). \\\n",
        "                                  cumcount()+1\n",
        "  # 2-3-2) diff 관련 \n",
        "  # contents 기준 \n",
        "  for i in intervals:\n",
        "    df['contents_open_{}_timediff'.format(i)] = df.\\\n",
        "        sort_values('contents_open_dt', ascending=False). \\\n",
        "        groupby([pd.Grouper('contents_rn'), pd.Grouper(freq=i, key='contents_open_dt')]) \\\n",
        "        ['contents_open_dt'].diff(-1) \\\n",
        "        .dt.seconds.div(60).fillna(0)   # 초 단위로 환산, 날짜형은 결측치를 0으로 대체 불가능하기에 환산함\n",
        "  # person 기준\n",
        "  for i in intervals:  \n",
        "    df['person_open_{}_timediff'.format(i)] = df.\\\n",
        "        sort_values('contents_open_dt', ascending=False). \\\n",
        "        groupby([pd.Grouper('person_rn'), pd.Grouper(freq=i, key='contents_open_dt')]) \\\n",
        "        ['contents_open_dt'].diff(-1) \\\n",
        "        .dt.seconds.div(60).fillna(0)  \n",
        "\n",
        "\n",
        "  # 3) 불필요한 열 제거 \n",
        "  del_cols_list = [c for c in df.columns if c in \n",
        "                    ['id','contents_open_dt',#'contents_rn','person_rn',\n",
        "                     'person_prefer_f','person_prefer_g',\n",
        "                     'd_l_match_yn','d_m_match_yn','d_s_match_yn',\n",
        "                     'h_l_match_yn','h_m_match_yn','h_s_match_yn']\n",
        "  ]\n",
        "  df.drop(del_cols_list, axis=\"columns\",inplace=True) \n",
        "\n",
        "  return id_list, df \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PyvWxEjlCip-"
      },
      "outputs": [],
      "source": [
        "def preprocessing_3_test(df=test):\n",
        "\n",
        "  # 1) id 저장 \n",
        "  id_list = list(df['id'])\n",
        "\n",
        "  # 2-1) 코드 매칭 관련 파생변수 생성 (add_code)\n",
        "  D_attr_types = ['세', '소', '중', '대']\n",
        "  H_attr_types = ['중', '대']\n",
        "  L_attr_types = ['세', '소', '중', '대']\n",
        "  prefer_rank = ['1', '2', '3'] \n",
        "\n",
        "  ## D 속성 코드 \n",
        "  for D_type in D_attr_types:\n",
        "    # 회원 속성 D\n",
        "    for rank in prefer_rank:\n",
        "      df['person_prefer_d_{}_{}'.format(rank, D_type)] = df['person_prefer_d_{}'.format(rank)].apply(lambda x:attr_D_code[x]['속성 D {}분류코드'.format(D_type)])\n",
        "    # 컨텐츠 속성 D \n",
        "    df['contents_attribute_d_{}'.format(D_type)] = df['contents_attribute_d'].apply(lambda x:attr_D_code[x]['속성 D {}분류코드'.format(D_type)])\n",
        "\n",
        "  ## H 속성 코드 \n",
        "  for H_type in H_attr_types:\n",
        "    # 회원 속성 H\n",
        "    for rank in prefer_rank:\n",
        "      df['person_prefer_h_{}_{}'.format(rank, H_type)] = df['person_prefer_h_{}'.format(rank)].apply(lambda x:attr_H_code[x]['속성 H {}분류코드'.format(H_type)])\n",
        "    # 컨텐츠 속성 H \n",
        "    df['contents_attribute_h_{}'.format(H_type)] = df['contents_attribute_h'].apply(lambda x:attr_H_code[x]['속성 H {}분류코드'.format(H_type)])\n",
        "\n",
        "  ## L 속성 코드 \n",
        "  for L_type in L_attr_types:\n",
        "    # 컨텐츠 속성 L \n",
        "    df['contents_attribute_l_{}'.format(L_type)] = df['contents_attribute_l'].apply(lambda x:attr_L_code[x]['속성 L {}분류코드'.format(L_type)])\n",
        "\n",
        "\n",
        "  # 2-2) person-contents 일치 여부 파생변수 생성 \n",
        "  cols_equal = [\n",
        "          ('person_prefer_c','contents_attribute_c'),\n",
        "\n",
        "          ('person_prefer_d_1_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_1_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_1_소','contents_attribute_d_소'),\n",
        "          ('person_prefer_d_1_세','contents_attribute_d_세'),\n",
        "          ('person_prefer_d_1','contents_attribute_d'),\n",
        "\n",
        "          ('person_prefer_d_2_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_2_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_2_소','contents_attribute_d_소'),\n",
        "\n",
        "          ('person_prefer_d_3_대','contents_attribute_d_대'),\n",
        "          ('person_prefer_d_3_중','contents_attribute_d_중'),\n",
        "          ('person_prefer_d_3_소','contents_attribute_d_소'),\n",
        "\n",
        "          ('person_prefer_h_1_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_1_중','contents_attribute_h_중'),\n",
        "          ('person_prefer_h_1','contents_attribute_h'),\n",
        "\n",
        "          ('person_prefer_h_2_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_2_중','contents_attribute_h_중'),\n",
        "\n",
        "          ('person_prefer_h_3_대','contents_attribute_h_대'),\n",
        "          ('person_prefer_h_3_중','contents_attribute_h_중')\n",
        "        \n",
        "  ]\n",
        "\n",
        "  for col1, col2 in cols_equal:\n",
        "    df[f\"{col1}_{col2}\"]=(df[col1]==df[col2]).astype(int)\n",
        "\n",
        "  df['person_attribute_a_contents_attribute_a'] = np.where(((df['person_attribute_a'] == df['contents_attribute_a'])|(df['contents_attribute_a']==3)),1,0)  # EDA 결과 3인 경우는 '상관없음'으로 판단함\n",
        "  df['person_attribute_e_contents_attribute_e'] = np.where(((df['person_prefer_e'] == df['contents_attribute_e'])|(df['contents_attribute_e']==0)),1,0)  # EDA 결과 0인 경우는 '상관없음'으로 판단함 \n",
        "\n",
        "  # 2-3) cumdiff 관련 파생변수 생성 \n",
        "\n",
        "  # 데이터형 변환: object -> datetime\n",
        "  df['contents_open_dt'] = pd.to_datetime(df['contents_open_dt'])\n",
        "  \n",
        "  intervals = ['D', 'W', 'M']\n",
        "  vartypes = ['cumct', 'timediff']\n",
        "\n",
        "  contents_df = pd.DataFrame()\n",
        "  person_df = pd.DataFrame()\n",
        "\n",
        "  for i in vartypes: \n",
        "    for j in intervals: \n",
        "      contents_df['contents_open_{}_{}'.format(j,i)] = train3. \\\n",
        "          groupby('contents_rn')['contents_open_{}_{}'.format(j,i)].max()\n",
        "\n",
        "  for i in vartypes: \n",
        "    for j in intervals: \n",
        "      person_df['person_open_{}_{}'.format(j,i)] = train3. \\\n",
        "          groupby('person_rn')['person_open_{}_{}'.format(j,i)].max()\n",
        "\n",
        "  contents_df.reset_index(level=0, inplace=True)\n",
        "  person_df.reset_index(level=0, inplace=True)\n",
        "\n",
        "  df = df.merge(contents_df, how='left', on='contents_rn').fillna(0) \\\n",
        "              .merge(person_df, how='left', on='person_rn').fillna(0)\n",
        "\n",
        "  # 3) 불필요한 열 제거 \n",
        "  del_cols_list = [c for c in df.columns if c in \n",
        "                    ['id','contents_rn','person_rn','contents_open_dt',\n",
        "                     'person_prefer_f','person_prefer_g',\n",
        "                     'd_l_match_yn','d_m_match_yn','d_s_match_yn',\n",
        "                     'h_l_match_yn','h_m_match_yn','h_s_match_yn']\n",
        "  ]\n",
        "  df.drop(del_cols_list, axis=\"columns\",inplace=True) \n",
        "\n",
        "  return id_list, df \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boodTCfQp_SI"
      },
      "source": [
        "#### `preprocessing_3` 전처리 함수 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mRk5WMo_zMFh"
      },
      "outputs": [],
      "source": [
        "# 파일 불러오기 \n",
        "train = pd.read_csv('Jobcare_data/train.csv')\n",
        "test = pd.read_csv('Jobcare_data/test.csv')\n",
        "\n",
        "# 3타입 전처리 함수 적용하기\n",
        "train3_idx, train3 = preprocessing_3_train(train)\n",
        "test3_idx, test3 = preprocessing_3_test(test)\n",
        "\n",
        "train3 = train3.drop(['contents_rn','person_rn'], axis=1)\n",
        "#train3.to_csv('train3.csv', header=True,index=False)\n",
        "#test3.to_csv('test3.csv', header=True,index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XDT1obetCcd7"
      },
      "outputs": [],
      "source": [
        "numeric_columns3 = [\n",
        "  'contents_attribute_j','contents_attribute_k',\n",
        "\n",
        "  'contents_open_D_cumct','contents_open_W_cumct', 'contents_open_M_cumct',\n",
        "  'contents_open_D_timediff', 'contents_open_W_timediff','contents_open_M_timediff', \n",
        "  'person_open_D_cumct', 'person_open_W_cumct', 'person_open_M_cumct', \n",
        "  'person_open_D_timediff','person_open_W_timediff', 'person_open_M_timediff',\n",
        "\n",
        "  'person_prefer_c_contents_attribute_c',\n",
        "  'person_prefer_d_1_대_contents_attribute_d_대',\n",
        "  'person_prefer_d_1_중_contents_attribute_d_중',\n",
        "  'person_prefer_d_1_소_contents_attribute_d_소',\n",
        "  'person_prefer_d_1_세_contents_attribute_d_세',\n",
        "  'person_prefer_d_1_contents_attribute_d',\n",
        "  'person_prefer_d_2_대_contents_attribute_d_대',\n",
        "  'person_prefer_d_2_중_contents_attribute_d_중',\n",
        "  'person_prefer_d_2_소_contents_attribute_d_소',\n",
        "  'person_prefer_d_3_대_contents_attribute_d_대',\n",
        "  'person_prefer_d_3_중_contents_attribute_d_중',\n",
        "  'person_prefer_d_3_소_contents_attribute_d_소',\n",
        "  'person_prefer_h_1_대_contents_attribute_h_대',\n",
        "  'person_prefer_h_1_중_contents_attribute_h_중',\n",
        "  'person_prefer_h_1_contents_attribute_h',\n",
        "  'person_prefer_h_2_대_contents_attribute_h_대',\n",
        "  'person_prefer_h_2_중_contents_attribute_h_중',\n",
        "  'person_prefer_h_3_대_contents_attribute_h_대',\n",
        "  'person_prefer_h_3_중_contents_attribute_h_중',\n",
        "  'person_attribute_a_contents_attribute_a',\n",
        "  'person_attribute_e_contents_attribute_e'\n",
        "]\n",
        "\n",
        "categorical_columns3 = list(train3.columns.drop(numeric_columns3 + ['target']))\n",
        "feature_names3 = numeric_columns3 + categorical_columns3\n",
        "\n",
        "train3[categorical_columns3]=train3[categorical_columns3].astype('category')\n",
        "test3[categorical_columns3]=test3[categorical_columns3].astype('category')\n",
        "train3[numeric_columns3]=train3[numeric_columns3].astype('int')\n",
        "test3[numeric_columns3]=test3[numeric_columns3].astype('int')\n",
        "f_labels3 = categorical_columns3+numeric_columns3\n",
        "\n",
        "# X, y 나누기 (train만 적용)\n",
        "y_train3 = train3['target']\n",
        "X_train3 = train3.loc[:,train3.columns != 'target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sZNdAVC63de"
      },
      "source": [
        "## 3. 모델링 (Modeling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnWH6rxQ7Eg9"
      },
      "source": [
        "### **3-1. CatBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UtdJdXdw7BST"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=4, shuffle=True, random_state=2022)    # K-Fold 4개로 cross validation 진행\n",
        "\n",
        "cat_train1_model=[]\n",
        "cat_train2_model=[]\n",
        "cat_train3_model=[]\n",
        "index_number_validation=[]  \n",
        "validation_target=[] \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "각각의 train data(`X_train1`, `X_train2`, `X_train3`)에 대한 catboost model을 적합시키고 저장한다."
      ],
      "metadata": {
        "id": "q8g8d8Q-TWUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NKtCk6Ahlwrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83aa4361-0b69-4266-9047-7d556ccb6a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.6252440\ttest: 0.6238603\tbest: 0.6238603 (0)\ttotal: 114ms\tremaining: 5m 43s\n",
            "100:\tlearn: 0.6633477\ttest: 0.6737210\tbest: 0.6742372 (94)\ttotal: 8.18s\tremaining: 3m 54s\n",
            "200:\tlearn: 0.6702190\ttest: 0.6734295\tbest: 0.6753422 (120)\ttotal: 15.7s\tremaining: 3m 38s\n",
            "300:\tlearn: 0.6735839\ttest: 0.6722207\tbest: 0.6753422 (120)\ttotal: 23.1s\tremaining: 3m 26s\n",
            "bestTest = 0.6753421918\n",
            "bestIteration = 120\n",
            "Shrink model to first 121 iterations.\n",
            "0:\tlearn: 0.6282091\ttest: 0.6236108\tbest: 0.6236108 (0)\ttotal: 103ms\tremaining: 5m 9s\n",
            "100:\tlearn: 0.6649003\ttest: 0.6731015\tbest: 0.6734552 (99)\ttotal: 8.13s\tremaining: 3m 53s\n",
            "200:\tlearn: 0.6706141\ttest: 0.6748834\tbest: 0.6758384 (176)\ttotal: 15.7s\tremaining: 3m 38s\n",
            "300:\tlearn: 0.6747654\ttest: 0.6747210\tbest: 0.6758384 (176)\ttotal: 23.1s\tremaining: 3m 27s\n",
            "400:\tlearn: 0.6779339\ttest: 0.6741175\tbest: 0.6758384 (176)\ttotal: 30.6s\tremaining: 3m 18s\n",
            "bestTest = 0.6758384111\n",
            "bestIteration = 176\n",
            "Shrink model to first 177 iterations.\n",
            "0:\tlearn: 0.5931651\ttest: 0.5946856\tbest: 0.5946856 (0)\ttotal: 102ms\tremaining: 5m 6s\n",
            "100:\tlearn: 0.6654193\ttest: 0.6711362\tbest: 0.6722025 (99)\ttotal: 8.27s\tremaining: 3m 57s\n",
            "200:\tlearn: 0.6708242\ttest: 0.6733179\tbest: 0.6734915 (198)\ttotal: 15.7s\tremaining: 3m 39s\n",
            "300:\tlearn: 0.6739022\ttest: 0.6735279\tbest: 0.6741770 (244)\ttotal: 23.3s\tremaining: 3m 28s\n",
            "400:\tlearn: 0.6774780\ttest: 0.6727188\tbest: 0.6741770 (244)\ttotal: 30.8s\tremaining: 3m 19s\n",
            "bestTest = 0.6741770354\n",
            "bestIteration = 244\n",
            "Shrink model to first 245 iterations.\n",
            "0:\tlearn: 0.6319598\ttest: 0.6290624\tbest: 0.6290624 (0)\ttotal: 105ms\tremaining: 5m 13s\n",
            "100:\tlearn: 0.6642171\ttest: 0.6731842\tbest: 0.6748218 (71)\ttotal: 8.17s\tremaining: 3m 54s\n",
            "200:\tlearn: 0.6700868\ttest: 0.6736938\tbest: 0.6753982 (133)\ttotal: 15.7s\tremaining: 3m 39s\n",
            "300:\tlearn: 0.6738411\ttest: 0.6761635\tbest: 0.6762767 (292)\ttotal: 23.2s\tremaining: 3m 28s\n",
            "400:\tlearn: 0.6764534\ttest: 0.6763459\tbest: 0.6765894 (391)\ttotal: 30.6s\tremaining: 3m 18s\n",
            "500:\tlearn: 0.6790847\ttest: 0.6763516\tbest: 0.6775183 (458)\ttotal: 37.9s\tremaining: 3m 9s\n",
            "600:\tlearn: 0.6815666\ttest: 0.6771314\tbest: 0.6775183 (458)\ttotal: 45.3s\tremaining: 3m\n",
            "700:\tlearn: 0.6835999\ttest: 0.6757991\tbest: 0.6775183 (458)\ttotal: 52.5s\tremaining: 2m 52s\n",
            "bestTest = 0.6775183183\n",
            "bestIteration = 458\n",
            "Shrink model to first 459 iterations.\n",
            "0:\tlearn: 0.6151258\ttest: 0.6137276\tbest: 0.6137276 (0)\ttotal: 116ms\tremaining: 5m 48s\n",
            "100:\tlearn: 0.6642737\ttest: 0.6722073\tbest: 0.6727493 (98)\ttotal: 9.96s\tremaining: 4m 45s\n",
            "200:\tlearn: 0.6705584\ttest: 0.6736375\tbest: 0.6748371 (154)\ttotal: 19.3s\tremaining: 4m 28s\n",
            "300:\tlearn: 0.6737790\ttest: 0.6744626\tbest: 0.6748371 (154)\ttotal: 28.4s\tremaining: 4m 14s\n",
            "400:\tlearn: 0.6766167\ttest: 0.6736208\tbest: 0.6754560 (334)\ttotal: 37.4s\tremaining: 4m 2s\n",
            "500:\tlearn: 0.6795326\ttest: 0.6734162\tbest: 0.6754560 (334)\ttotal: 46.4s\tremaining: 3m 51s\n",
            "bestTest = 0.6754559997\n",
            "bestIteration = 334\n",
            "Shrink model to first 335 iterations.\n",
            "0:\tlearn: 0.6171355\ttest: 0.6170198\tbest: 0.6170198 (0)\ttotal: 98ms\tremaining: 4m 54s\n",
            "100:\tlearn: 0.6643361\ttest: 0.6701479\tbest: 0.6719065 (76)\ttotal: 10.1s\tremaining: 4m 51s\n",
            "200:\tlearn: 0.6711369\ttest: 0.6728168\tbest: 0.6731252 (149)\ttotal: 19.4s\tremaining: 4m 30s\n",
            "300:\tlearn: 0.6742577\ttest: 0.6737464\tbest: 0.6739150 (291)\ttotal: 28.6s\tremaining: 4m 16s\n",
            "400:\tlearn: 0.6772683\ttest: 0.6742656\tbest: 0.6742664 (382)\ttotal: 37.8s\tremaining: 4m 5s\n",
            "500:\tlearn: 0.6803044\ttest: 0.6745287\tbest: 0.6746484 (498)\ttotal: 47.1s\tremaining: 3m 54s\n",
            "600:\tlearn: 0.6823970\ttest: 0.6750400\tbest: 0.6754175 (549)\ttotal: 56s\tremaining: 3m 43s\n",
            "700:\tlearn: 0.6842925\ttest: 0.6753045\tbest: 0.6754175 (549)\ttotal: 1m 5s\tremaining: 3m 33s\n",
            "800:\tlearn: 0.6865802\ttest: 0.6743312\tbest: 0.6755436 (709)\ttotal: 1m 14s\tremaining: 3m 23s\n",
            "900:\tlearn: 0.6889888\ttest: 0.6747842\tbest: 0.6755436 (709)\ttotal: 1m 23s\tremaining: 3m 13s\n",
            "bestTest = 0.6755435992\n",
            "bestIteration = 709\n",
            "Shrink model to first 710 iterations.\n",
            "0:\tlearn: 0.6202164\ttest: 0.6190809\tbest: 0.6190809 (0)\ttotal: 135ms\tremaining: 6m 44s\n",
            "100:\tlearn: 0.6654328\ttest: 0.6732765\tbest: 0.6737958 (97)\ttotal: 10.1s\tremaining: 4m 49s\n",
            "200:\tlearn: 0.6718175\ttest: 0.6744925\tbest: 0.6744925 (200)\ttotal: 19.5s\tremaining: 4m 31s\n",
            "300:\tlearn: 0.6748664\ttest: 0.6740159\tbest: 0.6744925 (200)\ttotal: 28.5s\tremaining: 4m 15s\n",
            "400:\tlearn: 0.6775198\ttest: 0.6741422\tbest: 0.6747458 (347)\ttotal: 37.9s\tremaining: 4m 5s\n",
            "500:\tlearn: 0.6798093\ttest: 0.6738147\tbest: 0.6747458 (347)\ttotal: 47.6s\tremaining: 3m 57s\n",
            "bestTest = 0.6747458029\n",
            "bestIteration = 347\n",
            "Shrink model to first 348 iterations.\n",
            "0:\tlearn: 0.6354574\ttest: 0.6367346\tbest: 0.6367346 (0)\ttotal: 113ms\tremaining: 5m 37s\n",
            "100:\tlearn: 0.6644269\ttest: 0.6769654\tbest: 0.6771605 (98)\ttotal: 10s\tremaining: 4m 47s\n",
            "200:\tlearn: 0.6702219\ttest: 0.6797673\tbest: 0.6803329 (163)\ttotal: 19.5s\tremaining: 4m 31s\n",
            "300:\tlearn: 0.6740519\ttest: 0.6786499\tbest: 0.6803329 (163)\ttotal: 28.5s\tremaining: 4m 15s\n",
            "400:\tlearn: 0.6772962\ttest: 0.6769724\tbest: 0.6803329 (163)\ttotal: 37.6s\tremaining: 4m 3s\n",
            "bestTest = 0.680332944\n",
            "bestIteration = 163\n",
            "Shrink model to first 164 iterations.\n",
            "0:\tlearn: 0.6110134\ttest: 0.6120298\tbest: 0.6120298 (0)\ttotal: 131ms\tremaining: 6m 32s\n",
            "100:\tlearn: 0.6624826\ttest: 0.6749039\tbest: 0.6757080 (96)\ttotal: 11s\tremaining: 5m 14s\n",
            "200:\tlearn: 0.6705609\ttest: 0.6796354\tbest: 0.6797565 (183)\ttotal: 20.9s\tremaining: 4m 50s\n",
            "300:\tlearn: 0.6749278\ttest: 0.6796838\tbest: 0.6806881 (230)\ttotal: 30.4s\tremaining: 4m 32s\n",
            "400:\tlearn: 0.6768467\ttest: 0.6803020\tbest: 0.6806881 (230)\ttotal: 40.3s\tremaining: 4m 21s\n",
            "bestTest = 0.6806880902\n",
            "bestIteration = 230\n",
            "Shrink model to first 231 iterations.\n",
            "0:\tlearn: 0.6236427\ttest: 0.6263917\tbest: 0.6263917 (0)\ttotal: 111ms\tremaining: 5m 34s\n",
            "100:\tlearn: 0.6641486\ttest: 0.6759549\tbest: 0.6759549 (100)\ttotal: 10.8s\tremaining: 5m 8s\n",
            "200:\tlearn: 0.6709656\ttest: 0.6765116\tbest: 0.6769441 (173)\ttotal: 20.5s\tremaining: 4m 46s\n",
            "300:\tlearn: 0.6751303\ttest: 0.6762267\tbest: 0.6769441 (173)\ttotal: 30.2s\tremaining: 4m 31s\n",
            "400:\tlearn: 0.6775609\ttest: 0.6762724\tbest: 0.6769460 (354)\ttotal: 39.6s\tremaining: 4m 16s\n",
            "500:\tlearn: 0.6802076\ttest: 0.6769493\tbest: 0.6771379 (456)\ttotal: 49.1s\tremaining: 4m 4s\n",
            "600:\tlearn: 0.6827695\ttest: 0.6774404\tbest: 0.6775078 (586)\ttotal: 58.5s\tremaining: 3m 53s\n",
            "700:\tlearn: 0.6850542\ttest: 0.6770326\tbest: 0.6777981 (627)\ttotal: 1m 8s\tremaining: 3m 43s\n",
            "800:\tlearn: 0.6873291\ttest: 0.6766211\tbest: 0.6777981 (627)\ttotal: 1m 17s\tremaining: 3m 32s\n",
            "bestTest = 0.6777981059\n",
            "bestIteration = 627\n",
            "Shrink model to first 628 iterations.\n",
            "0:\tlearn: 0.6250680\ttest: 0.6205170\tbest: 0.6205170 (0)\ttotal: 133ms\tremaining: 6m 39s\n",
            "100:\tlearn: 0.6637083\ttest: 0.6703297\tbest: 0.6706918 (97)\ttotal: 10.6s\tremaining: 5m 4s\n",
            "200:\tlearn: 0.6714672\ttest: 0.6769243\tbest: 0.6770781 (195)\ttotal: 20.6s\tremaining: 4m 46s\n",
            "300:\tlearn: 0.6752870\ttest: 0.6784000\tbest: 0.6785914 (293)\ttotal: 30.2s\tremaining: 4m 30s\n",
            "400:\tlearn: 0.6780177\ttest: 0.6767797\tbest: 0.6790596 (368)\ttotal: 39.9s\tremaining: 4m 18s\n",
            "500:\tlearn: 0.6804960\ttest: 0.6764590\tbest: 0.6790596 (368)\ttotal: 49.9s\tremaining: 4m 8s\n",
            "600:\tlearn: 0.6832709\ttest: 0.6758513\tbest: 0.6790596 (368)\ttotal: 59.4s\tremaining: 3m 57s\n",
            "bestTest = 0.6790595988\n",
            "bestIteration = 368\n",
            "Shrink model to first 369 iterations.\n",
            "0:\tlearn: 0.6193003\ttest: 0.6234700\tbest: 0.6234700 (0)\ttotal: 133ms\tremaining: 6m 38s\n",
            "100:\tlearn: 0.6634924\ttest: 0.6780591\tbest: 0.6780591 (100)\ttotal: 11s\tremaining: 5m 14s\n",
            "200:\tlearn: 0.6705285\ttest: 0.6795531\tbest: 0.6801505 (168)\ttotal: 21s\tremaining: 4m 52s\n",
            "300:\tlearn: 0.6747437\ttest: 0.6795700\tbest: 0.6812378 (241)\ttotal: 30.8s\tremaining: 4m 35s\n",
            "400:\tlearn: 0.6777828\ttest: 0.6791496\tbest: 0.6812378 (241)\ttotal: 40.3s\tremaining: 4m 21s\n",
            "bestTest = 0.6812378368\n",
            "bestIteration = 241\n",
            "Shrink model to first 242 iterations.\n"
          ]
        }
      ],
      "source": [
        "# Modeling Fitting & Saving\n",
        "\n",
        "## X_train1\n",
        "for i, j in cv.split(X_train1):\n",
        "  index_number_validation.append(j)       # cv를 shuffle 시켜 index number를 저장해줌\n",
        "  catboost_clf=CatBoostClassifier(cat_features = categorical_columns1, l2_leaf_reg=120, \n",
        "                               depth=6, auto_class_weights='Balanced',iterations=3000,\n",
        "                               learning_rate=0.2, use_best_model=True, \n",
        "                               early_stopping_rounds=250, eval_metric='F1',\n",
        "                               random_state=2022,one_hot_max_size=5,\n",
        "                               task_type=\"GPU\")\n",
        "  catboost_clf.fit(X_train1.iloc[i], y_train1[i],\n",
        "                   eval_set=[(X_train1.iloc[j],y_train1[j])],\n",
        "                   early_stopping_rounds=250,\n",
        "                   verbose=100)\n",
        "  cat_train1_model.append(catboost_clf)  # train1으로 fit 한 model 저장\n",
        "  if False:\n",
        "    break\n",
        "\n",
        "## X_train2\n",
        "for i,j in cv.split(X_train2):  \n",
        "  catboost_clf=CatBoostClassifier(cat_features = categorical_columns2, l2_leaf_reg=120, \n",
        "                               depth=6, auto_class_weights='Balanced',iterations=3000,\n",
        "                               learning_rate=0.2, use_best_model=True, \n",
        "                               early_stopping_rounds=250, eval_metric='F1',\n",
        "                               random_state=2022,one_hot_max_size=5,\n",
        "                               task_type=\"GPU\")\n",
        "  catboost_clf.fit(X_train2.iloc[i], y_train2[i],\n",
        "                   eval_set=[(X_train2.iloc[j],y_train2[j])],\n",
        "                   early_stopping_rounds=250,\n",
        "                   verbose=100)  # 100회마다 score 출력 \n",
        "  cat_train2_model.append(catboost_clf)   # train2로 fit한 model 저장\n",
        "  if False:\n",
        "    break\n",
        "\n",
        "## X_train3\n",
        "for i,j in cv.split(X_train3):  \n",
        "  catboost_clf=CatBoostClassifier(cat_features = categorical_columns3, l2_leaf_reg=120, \n",
        "                               depth=6, auto_class_weights='Balanced',iterations=3000,\n",
        "                               learning_rate=0.2, use_best_model=True, \n",
        "                               early_stopping_rounds=250, eval_metric='F1',\n",
        "                               random_state=2022,one_hot_max_size=5,\n",
        "                               task_type=\"GPU\")\n",
        "  catboost_clf.fit(X_train3.iloc[i], y_train3[i],\n",
        "                   eval_set=[(X_train3.iloc[j],y_train3[j])],\n",
        "                   early_stopping_rounds=250,\n",
        "                   verbose=100)\n",
        "  cat_train3_model.append(catboost_clf)   # train3로 fit한 model 저장\n",
        "  validation_target.append(y_train1[j])    # stacking을 위해 validation target 값 저장\n",
        "  if False:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpOJpKpCmznT"
      },
      "source": [
        "각각의 train data에 대하여 저장한 catboost model로 각 fold별 validation probability & test probability 값을 저장한다.\n",
        "stacking 단계에서 사용하는 모델을위하여 validation probability도 모두 저장하였다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QyUd0JEulwjU"
      },
      "outputs": [],
      "source": [
        "cat_train1_validation_prob=[]\n",
        "cat_train2_validation_prob=[]\n",
        "cat_train3_validation_prob=[]\n",
        "cat_train1_test_prob=[]\n",
        "cat_train2_test_prob=[]\n",
        "cat_train3_test_prob=[]\n",
        "\n",
        "for a,(i, j) in enumerate(cv.split(X_train1)):  \n",
        "  val_prob1=cat_train1_model[a].predict_proba(X_train1.iloc[j])[:,1]\n",
        "  test_prob1=cat_train1_model[a].predict_proba(test1)[:,1]\n",
        "  cat_train1_validation_prob.append(val_prob1)\n",
        "  cat_train1_test_prob.append(test_prob1)\n",
        "\n",
        "for a,(i,j) in enumerate(cv.split(X_train2)):\n",
        "  val_prob2=cat_train2_model[a].predict_proba(X_train2.iloc[j])[:,1]\n",
        "  test_prob2=cat_train2_model[a].predict_proba(test2)[:,1]\n",
        "  cat_train2_validation_prob.append(val_prob2)\n",
        "  cat_train2_test_prob.append(test_prob2)\n",
        "\n",
        "for a,(i,j) in enumerate(cv.split(X_train3)):\n",
        "  val_prob3=cat_train3_model[a].predict_proba(X_train3.iloc[j])[:,1]\n",
        "  test_prob3=cat_train3_model[a].predict_proba(test3)[:,1]\n",
        "  cat_train3_validation_prob.append(val_prob3)\n",
        "  cat_train3_test_prob.append(test_prob3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4hYJbHU7Bu-"
      },
      "source": [
        "### **3-2. Ridge**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8TR004_w1NW"
      },
      "source": [
        "위의 CatBoost에서와 동일한 방식으로, 각각의 train data에 대하여 ridge model를 적합시키고 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Bqj3M_mw7CA_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge_train1_model=[]\n",
        "ridge_train2_model=[]\n",
        "ridge_train3_model=[]\n",
        "\n",
        "## X_train1\n",
        "for i, j in cv.split(X_train1):\n",
        "  ridge=Ridge()\n",
        "  ridge.fit(X_train1.iloc[i],y_train1[i])\n",
        "  ridge_train1_model.append(ridge)\n",
        "  if False:\n",
        "    break\n",
        "\n",
        "## X_train2\n",
        "for i,j in cv.split(X_train2):\n",
        "  ridge=Ridge()\n",
        "  ridge.fit(X_train2.iloc[i],y_train2[i])\n",
        "  ridge_train2_model.append(ridge)\n",
        "  if False:\n",
        "    break\n",
        "\n",
        "## X_train3\n",
        "for i,j in cv.split(X_train3):\n",
        "  ridge=Ridge()\n",
        "  ridge.fit(X_train3.iloc[i],y_train3[i])\n",
        "  ridge_train3_model.append(ridge)\n",
        "  if False:\n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNXLzULhw4HO"
      },
      "source": [
        "각각의 train data에 대하여 저장한 ridge model로 각 fold별 validation probability & test probability 값 저장 (stacking을 위한 모델 학습을 위하여 validation probability도 모두 저장함)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x-rn0W4lwu2-"
      },
      "outputs": [],
      "source": [
        "ridge_train1_validation_prob=[]\n",
        "ridge_train2_validation_prob=[]\n",
        "ridge_train3_validation_prob=[]\n",
        "ridge_train1_test_prob=[]\n",
        "ridge_train2_test_prob=[]\n",
        "ridge_train3_test_prob=[]\n",
        "\n",
        "## X_train1\n",
        "for a,(i, j) in enumerate(cv.split(X_train1)):  \n",
        "  val_prob1=ridge_train1_model[a].predict(X_train1.iloc[j])\n",
        "  test_prob1=ridge_train1_model[a].predict(test1)\n",
        "  ridge_train1_validation_prob.append(val_prob1)\n",
        "  ridge_train1_test_prob.append(test_prob1)\n",
        "\n",
        "## X_train2\n",
        "for a,(i,j) in enumerate(cv.split(X_train2)):\n",
        "  val_prob2=ridge_train2_model[a].predict(X_train2.iloc[j])\n",
        "  test_prob2=ridge_train2_model[a].predict(test2)\n",
        "  ridge_train2_validation_prob.append(val_prob2)\n",
        "  ridge_train2_test_prob.append(test_prob2)\n",
        "\n",
        "## X_train3\n",
        "for a,(i,j) in enumerate(cv.split(X_train3)):\n",
        "  val_prob3=ridge_train3_model[a].predict(X_train3.iloc[j])\n",
        "  test_prob3=ridge_train3_model[a].predict(test3)\n",
        "  ridge_train3_validation_prob.append(val_prob3)\n",
        "  ridge_train3_test_prob.append(test_prob3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXgA93mWxqL6"
      },
      "source": [
        "### **3-3. Validation & Test DF 형성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JUf0hWMLwuiZ"
      },
      "outputs": [],
      "source": [
        "# validation proba DF\n",
        "validation1=pd.DataFrame({'index': index_number_validation[0], 'cat1': cat_train1_validation_prob[0], 'cat2': cat_train2_validation_prob[0],'cat3': cat_train3_validation_prob[0],'ridge1':ridge_train1_validation_prob[0],'ridge2':ridge_train2_validation_prob[0],'ridge3':ridge_train3_validation_prob[0], 'target':validation_target[0]})\n",
        "validation2=pd.DataFrame({'index': index_number_validation[1], 'cat1': cat_train1_validation_prob[1], 'cat2': cat_train2_validation_prob[1],'cat3': cat_train3_validation_prob[1],'ridge1':ridge_train1_validation_prob[1],'ridge2':ridge_train2_validation_prob[1],'ridge3':ridge_train3_validation_prob[1], 'target':validation_target[1]})\n",
        "validation3=pd.DataFrame({'index': index_number_validation[2], 'cat1': cat_train1_validation_prob[2], 'cat2': cat_train2_validation_prob[2],'cat3': cat_train3_validation_prob[2],'ridge1':ridge_train1_validation_prob[2],'ridge2':ridge_train2_validation_prob[2],'ridge3':ridge_train3_validation_prob[2], 'target':validation_target[2]})\n",
        "validation4=pd.DataFrame({'index': index_number_validation[3], 'cat1': cat_train1_validation_prob[3], 'cat2': cat_train2_validation_prob[3],'cat3': cat_train3_validation_prob[3],'ridge1':ridge_train1_validation_prob[3],'ridge2':ridge_train2_validation_prob[3],'ridge3':ridge_train3_validation_prob[3], 'target':validation_target[3]})\n",
        "validation_proba=pd.concat([validation1,validation2,validation3,validation4],axis=0)\n",
        "validation_proba.sort_values(by=['index'],inplace=True)  # cat validation prob DataFrame 저장\n",
        "\n",
        "# test proba DF\n",
        "test_proba=pd.DataFrame({'cat1':np.mean(cat_train1_test_prob,axis=0),'cat2':np.mean(cat_train2_test_prob,axis=0),'cat3':np.mean(cat_train3_test_prob,axis=0),'ridge1':np.mean(ridge_train1_test_prob,axis=0),'ridge2':np.mean(ridge_train2_test_prob,axis=0),'ridge3':np.mean(ridge_train3_test_prob,axis=0)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmS-u-OP7GQa"
      },
      "source": [
        "## 4. 스태킹 (Stacking)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOGlIMsl7JEm"
      },
      "source": [
        "Bayes Classifier를 메타 모델로 사용하였다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LaFD5S-q7FlW"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "import math\n",
        "import scipy.stats\n",
        "\n",
        "# bayes optimal classifier 분포값 정의\n",
        "def Bayes_optimal_classifier(data,mu,sigma_metrics):\n",
        "  # proba의 분포 가정\n",
        "  distribution_data=scipy.stats.multivariate_normal.pdf(data,mean=mu,cov=sigma_metrics)\n",
        "  return distribution_data\n",
        "\n",
        "validation_0=validation_proba[validation_proba['target']==0]\n",
        "validation_1=validation_proba[validation_proba['target']==1]\n",
        "validation_0.drop(['index','ridge3','target'],axis=1,inplace=True)\n",
        "validation_1.drop(['index','ridge3','target'],axis=1,inplace=True)\n",
        "\n",
        "x0, xm0, sig0, cov0=[0]*len(validation_0.columns), [0]*len(validation_0.columns), [0]*len(validation_0.columns), [0]*len(validation_0.columns)\n",
        "columns0=validation_0.columns\n",
        "for i in range(len(validation_0.columns)):\n",
        "  x0[i]=np.array(validation_0[columns0[i]])\n",
        "  xm0[i]=np.mean(x0[i])\n",
        "  sig0[i]=np.var(x0[i])\n",
        "for i in range(len(validation_0.columns)):\n",
        "  k=[0]*len(validation_0.columns)\n",
        "  for j in range(len(validation_0.columns)):\n",
        "    if i==j:\n",
        "      k[j]=sig0[j]\n",
        "    else:\n",
        "      k[j]=np.cov(x0[i],x0[j])[0,1]\n",
        "  cov0[i]=k\n",
        "\n",
        "x1, xm1, sig1, cov1=[0]*len(validation_1.columns), [0]*len(validation_1.columns), [0]*len(validation_1.columns), [0]*len(validation_1.columns)\n",
        "columns1=validation_1.columns\n",
        "for i in range(len(validation_1.columns)):\n",
        "  x1[i]=np.array(validation_1[columns1[i]])\n",
        "  xm1[i]=np.mean(x1[i])\n",
        "  sig1[i]=np.var(x1[i])\n",
        "for i in range(len(validation_1.columns)):\n",
        "  k=[0]*len(validation_1.columns)\n",
        "  for j in range(len(validation_1.columns)):\n",
        "    if i==j:\n",
        "      k[j]=sig1[j]\n",
        "    else:\n",
        "      k[j]=np.cov(x1[i],x1[j])[0,1]\n",
        "  cov1[i]=k\n",
        "test_columns=test_proba.columns\n",
        "t=[0]*len(test_columns)\n",
        "for i in range(len(test_columns)):\n",
        "  t[i]=np.array(test_proba[test_columns[i]])\n",
        "\n",
        "test_proba_data = np.asarray(np.vstack((np.vstack(t))).T)\n",
        "y_prob0 = np.array(  [Bayes_optimal_classifier(np.array([xx,yy,zz,jj, aa]),xm0,cov0) \n",
        "                     for xx, yy, zz, jj, aa in zip(np.ravel(test_proba_data[:,0]), np.ravel(test_proba_data[:,1]), np.ravel(test_proba_data[:,2]), np.ravel(test_proba_data[:,3]), np.ravel(test_proba_data[:,4])) ] )\n",
        "y_prob1 = np.array(  [Bayes_optimal_classifier(np.array([xx,yy,zz,jj, aa]),xm1,cov1) \n",
        "                     for xx, yy, zz, jj, aa in zip(np.ravel(test_proba_data[:,0]), np.ravel(test_proba_data[:,1]), np.ravel(test_proba_data[:,2]), np.ravel(test_proba_data[:,3]), np.ravel(test_proba_data[:,4])) ] )\n",
        "prob_for_1=y_prob1/(y_prob0+y_prob1)\n",
        "\n",
        "treshold=0.27\n",
        "test_pred=np.where(prob_for_1>=treshold,1,0).flatten()\n",
        "\n",
        "# 최종 예측 결과\n",
        "final_prediction=pd.read_csv('Jobcare_data/sample_submission.csv')\n",
        "final_prediction['target'] = test_pred\n",
        "#final_prediction.to_csv('final_prediction.csv')  # 결과 저장 "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "끝."
      ],
      "metadata": {
        "id": "nibRa1thq0NK"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "[팀 아샷추]코드 제출본.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}